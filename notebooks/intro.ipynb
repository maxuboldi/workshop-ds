{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Workshop - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is data science?\n",
    "\n",
    "[\"Data science is the discipline of making data useful. [...] the idea of usefulness is tightly coupled with influencing real-world actions.\"](https://hackernoon.com/what-on-earth-is-data-science-eb1237d8cb37) (Cassie Kozyrkov - Chief Decision Scientist at Google)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://dss-www-production.s3.amazonaws.com/uploads/2018/12/Data-Science-fields-600x544.png\" width=\"450\" height=\"400\"/>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://hackernoon.com/hn-images/1*8Wz6lQ8GFEAvnSS5uqMQ5g.png\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRISP-DM (Cross Industry Standard Process for Data Mining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <a href=\"https://www.datascience-pm.com/crisp-dm-2/\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b9/CRISP-DM_Process_Diagram.png/440px-CRISP-DM_Process_Diagram.png\">\n",
    "  </a>\n",
    "</p>\n",
    "\n",
    "1. Business understanding – What does the business need?\n",
    "2. Data understanding – What data do we have / need? Is it clean?\n",
    "3. Data preparation – How do we organize the data for modeling?\n",
    "4. Modeling – What modeling techniques should we apply?\n",
    "5. Evaluation – Which model best meets the business objectives?\n",
    "6. Deployment – How do stakeholders access the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://www.researchgate.net/profile/T-Suryakanthi-2/publication/339639928/figure/fig1/AS:864826486161408@1583202110233/Broad-Classification-of-Machine-Learning-Techniques.png\" width=\"600\" height=\"400\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://slidetodoc.com/presentation_image_h/14ad450855ff729f2ab4d8ef873b6ba3/image-8.jpg\" width=\"600\" height=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an \"algorithm\" in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An \"algorithm\" in machine learning is a procedure that is run on data to create a machine learning \"model.\"\n",
    "\n",
    "Machine learning algorithms perform \"pattern recognition.\" Algorithms \"learn\" from data, or are \"fit\" on a dataset.\n",
    "\n",
    "There are many machine learning algorithms for classification, such as Decision Tree, for regression, such as linear regression, and for clustering, such as k-means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a \"model\" in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"model\" in machine learning is the output of a machine learning algorithm run on data.\n",
    "\n",
    "A model represents what was learned by a machine learning algorithm.\n",
    "\n",
    "The model is the \"thing\" that is saved after running a machine learning algorithm on training data and represents the rules, numbers, and any other algorithm-specific data structures required to make predictions.\n",
    "\n",
    "Some examples:\n",
    "\n",
    "- The linear regression algorithm results in a model comprised of a vector of coefficients with specific values.\n",
    "- The decision tree algorithm results in a model comprised of a tree of if-then statements with specific values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <a href=\"https://scikit-learn.org/stable/\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Scikit_learn_logo_small.svg/500px-Scikit_learn_logo_small.svg.png\">\n",
    "  </a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem we will solve is to convert from Celsius to Fahrenheit, where the approximate formula is:\n",
    "\n",
    "$$ f = c \\times 1.8 + 32 $$\n",
    "\n",
    "Of course, it would be simple enough to create a conventional Python function that directly performs this calculation, but that wouldn't be machine learning.\n",
    "\n",
    "Instead, we will train a model that figures out the above formula through the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is a regression model to find a linear function that expresses relationship between dependent and independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/max/1276/0*CoAF7U14zw5hRgvu.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "celsius_X = np.array([-40, -10,  0,  8, 15, 22,  38],\n",
    "                     dtype=float).reshape(-1, 1)\n",
    "fahrenheit_Y = np.array([-40, 14, 32, 46, 59, 72, 100], dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the algorithm\n",
    "reg = LinearRegression().fit(celsius_X, fahrenheit_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "reg.predict(np.array([[100]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer is $100 \\times 1.8 + 32 = 212$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reg.coef_)\n",
    "print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance measure is the way you want to evaluate a solution to the problem. It is the measurement you will make of the predictions made by a trained model on the test dataset and are typically specialized to the class of problem (classification, regression, clustering, etc.).\n",
    "\n",
    "For example, for regression, **_MAE_** (Mean Absolute Error):\n",
    "\n",
    "$$ MAE = \\frac{1}{n}\\sum_{i=1}^{n}|y^{real}_i-y^{pred}_i| $$\n",
    "\n",
    "For classification, **_accuracy_**:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://static.wixstatic.com/media/02a1ae_32cad84eaf3348059a8996d1b0f88627~mv2.jpg/v1/fill/w_597,h_416,al_c,q_90/02a1ae_32cad84eaf3348059a8996d1b0f88627~mv2.jpg\" width=\"300\" height=\"200\">\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "$$ Accuracy = \\frac{TP+TN}{TP+TN+FP+FN} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more real example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But firts... what is a Decision Tree?\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://miro.medium.com/max/410/1*3L1-LxytBXu_26s4Bk4dFg.png\" width=\"300\" height=\"200\">\n",
    "</p>\n",
    "\n",
    "A decision tree is a learning algorithm, which is utilized for both classification and regression tasks. It has a hierarchical, tree structure, which consists of a root node, branches, internal nodes and leaf nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### and... what is boosting?\n",
    "\n",
    "Traditionally, building a Machine Learning application consisted on taking a single learner, like a Logistic Regressor or a Decision Tree, feeding it data, and teaching it to perform a certain task through this data.\n",
    "\n",
    "Then **ensemble** methods were born, which involve using many learners to enhance the performance of any single one of them individually. These methods can be described as techniques that use a group of weak learners (those who on average achieve only slightly better results than a random model) together, in order to create a stronger, aggregated one.\n",
    "\n",
    "Boosting models fall inside this family of ensemble methods.\n",
    "\n",
    "Boosting, initially named _Hypothesis Boosting_, consists on the idea of filtering or weighting the data that is used to train our team of weak learners, so that each new learner gives more weight or is only trained with observations that have been poorly classified by the previous learners.\n",
    "\n",
    "By doing this our team of models learns to make accurate predictions on all kinds of data, not just on the most common or easy observations. Also, if one of the individual models is very bad at making predictions on some kind of observation, it does not matter, as the other N-1 models will most likely make up for it.\n",
    "\n",
    "Boosting should not be confused with Bagging, which is the other main family of ensemble methods: while in bagging the weak learners are trained in parallel using randomness, in boosting the learners are trained sequentially, in order to be able to perform the task of data weighting/filtering described in the previous paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem\n",
    "\n",
    "With the anonymized flow of customers on a bank's website, new conversions have to be predicted for a period of time.\n",
    "\n",
    "This was a [Kaggle](https://www.kaggle.com/competitions/banco-galicia-dataton-2019/overview/description) competition of the year 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries/modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# HistGradientBoostingClassifier is a implementation of a decision tree \n",
    "# boosting algorithm\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([\n",
    "    pd.read_csv(\"../data/pageviews.csv\", parse_dates=[\"FEC_EVENT\"]),\n",
    "    pd.read_csv(\"../data/pageviews_complemento.csv\",\n",
    "    parse_dates=[\"FEC_EVENT\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "dataset.describe(include=\"all\", datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The first thing we have to do is define how to structure the data and separate training and testing.\n",
    "\n",
    "2. Then, as the prediction we have to make is at the user level, we are going to group all their navigation so that we have the same number of rows as the users we have.\n",
    "\n",
    "3. Finally, for each of the explanatory variables that we have (PAGE, CONTENT_CATEGORY, CONTENT_CATEGORY_TOP, CONTENT_CATEGORY_BOTTOM, SITE_ID, ON_SITE_SEARCH_TERM) we will:\n",
    "\n",
    "    - Add their frequency of occurrence of each value of each of the variables,\n",
    "    - calculate the frequency ratio of each possible value in relation to all the values that the variable can take (ie: for PAGE = 1, we add the number of times the user visited PAGE 1 and then divide it by the total visits that made that user to all PAGE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[dataset[\"FEC_EVENT\"].dt.month < 6]\n",
    "print(f\"The minimum date is {data['FEC_EVENT'].min()} and the maximum date is \\\n",
    "{data['FEC_EVENT'].max()}. \\n\")\n",
    "train_data = []\n",
    "for c in data.drop([\"USER_ID\", \"FEC_EVENT\"], axis=1).columns:\n",
    "    print(\"Making\", c)\n",
    "    temp = pd.crosstab(data.USER_ID, data[c])\n",
    "    temp.columns = [c + \"_\" + str(v) for v in temp.columns]\n",
    "    train_data.append(temp.apply(lambda x: x / x.sum(), axis=1))\n",
    "train_data = pd.concat(train_data, axis=1)\n",
    "print(f\"\\nTrain shape is {train_data.shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[dataset[\"FEC_EVENT\"].dt.month.between(6, 9)]\n",
    "print(f\"The minimum date is {data['FEC_EVENT'].min()} and the maximum date is \\\n",
    "{data['FEC_EVENT'].max()}. \\n\")\n",
    "test_data = []\n",
    "for c in data.drop([\"USER_ID\", \"FEC_EVENT\"], axis=1).columns:\n",
    "    print(\"Making\", c)\n",
    "    temp = pd.crosstab(data.USER_ID, data[c])\n",
    "    temp.columns = [c + \"_\" + str(v) for v in temp.columns]\n",
    "    test_data.append(temp.apply(lambda x: x / x.sum(), axis=1))\n",
    "test_data = pd.concat(test_data, axis=1)\n",
    "print(f\"\\nTest shape is {test_data.shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_col = [col for col in train_data if col.startswith(\"PAGE\")]\n",
    "train_data[filter_col].iloc[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have both datasets built, we are going to filter them, keeping the columns that exist in both, in order to train and predict on the same attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(set(train_data.columns).intersection(set(test_data.columns)))\n",
    "train_data = train_data[features]\n",
    "test_data = test_data[features]\n",
    "print(f\"Train shape is {train_data.shape}.\")\n",
    "print(f\"Test shape is {test_data.shape}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the **conversiones.csv** file that has the target variable and that corresponds to the conversions made during 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_csv(\"../data/conversiones.csv\")\n",
    "target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset again but looking 3 months ahead to align the prediction with the desired time window.\n",
    "\n",
    "* train_data = 2018-01-01/2018-05-31, train_target = 2018-06-01/2018-09-30.\n",
    "* test_data = 2018-06-01/2018-09-30, train_target = 2018-10-01/2018-12-31. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = pd.Series(0, index=train_data.index)\n",
    "train_idx = set(target[target[\"mes\"].between(\n",
    "    6, 9)].USER_ID.unique()).intersection(set(train_data.index))\n",
    "train_target.loc[list(train_idx)] = 1\n",
    "\n",
    "test_target = pd.Series(0, index=test_data.index)\n",
    "test_idx = set(target[target[\"mes\"] > 9].USER_ID.unique()\n",
    "               ).intersection(set(test_data.index))\n",
    "test_target.loc[list(test_idx)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class distribution in train\")\n",
    "print(train_target.value_counts())\n",
    "\n",
    "print(\"\\nClass distribution in test\")\n",
    "print(test_target.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = HistGradientBoostingClassifier(\n",
    "    random_state=0).fit(train_data, train_target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithms in scikit-learn can predict with a confidence score (in some cases) or predict the target directly (by default it considers a cutoff point of 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.predict_proba(test_data)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.predict(test_data)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy Score: {}\".format(accuracy_score(test_target,\n",
    "learner.predict(test_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix(rows=real, columns=pred)\")\n",
    "print(\n",
    "    pd.DataFrame(\n",
    "        confusion_matrix(\n",
    "            test_target,\n",
    "            learner.predict(test_data)\n",
    "            ), columns=['NO', 'YES'], index=['NO', 'YES']\n",
    "            )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb1d205f21b70400a7534dc1c0f579143c905efa8f17e985bd1c77bbe8f5c0bf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('wds-9IgDSWEo-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
