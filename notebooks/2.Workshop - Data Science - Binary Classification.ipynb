{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Workshop - Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem\n",
    "\n",
    "With the anonymized flow of customers on a bank's website, new conversions have to be predicted for a period of time.\n",
    "\n",
    "This was a [Kaggle](https://www.kaggle.com/competitions/banco-galicia-dataton-2019/overview/description) competition of the year 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries/modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([\n",
    "    pd.read_csv(\"../data/pageviews.csv\", parse_dates=[\"FEC_EVENT\"]),\n",
    "    pd.read_csv(\"../data/pageviews_complemento.csv\",\n",
    "    parse_dates=[\"FEC_EVENT\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEC_EVENT</th>\n",
       "      <th>PAGE</th>\n",
       "      <th>CONTENT_CATEGORY</th>\n",
       "      <th>CONTENT_CATEGORY_TOP</th>\n",
       "      <th>CONTENT_CATEGORY_BOTTOM</th>\n",
       "      <th>SITE_ID</th>\n",
       "      <th>ON_SITE_SEARCH_TERM</th>\n",
       "      <th>USER_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-30 07:35:48</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-03-30 07:35:52</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-03-30 07:36:11</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-03-30 07:36:16</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-03-30 07:41:38</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-03-30 07:41:42</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-03-30 07:42:01</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-03-30 07:42:05</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-03-30 07:43:43</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-03-30 07:44:14</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            FEC_EVENT  PAGE  CONTENT_CATEGORY  CONTENT_CATEGORY_TOP  \\\n",
       "0 2018-03-30 07:35:48     1                 1                     1   \n",
       "1 2018-03-30 07:35:52     2                 2                     2   \n",
       "2 2018-03-30 07:36:11     3                 2                     2   \n",
       "3 2018-03-30 07:36:16     4                 2                     2   \n",
       "4 2018-03-30 07:41:38     5                 2                     2   \n",
       "5 2018-03-30 07:41:42     2                 2                     2   \n",
       "6 2018-03-30 07:42:01     3                 2                     2   \n",
       "7 2018-03-30 07:42:05     4                 2                     2   \n",
       "8 2018-03-30 07:43:43     3                 2                     2   \n",
       "9 2018-03-30 07:44:14     6                 2                     2   \n",
       "\n",
       "   CONTENT_CATEGORY_BOTTOM  SITE_ID  ON_SITE_SEARCH_TERM  USER_ID  \n",
       "0                        1        1                    1        0  \n",
       "1                        2        2                    1        0  \n",
       "2                        2        3                    1        0  \n",
       "3                        2        3                    1        0  \n",
       "4                        2        2                    1        0  \n",
       "5                        2        2                    1        0  \n",
       "6                        2        3                    1        0  \n",
       "7                        2        3                    1        0  \n",
       "8                        2        3                    1        0  \n",
       "9                        2        3                    1        0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEC_EVENT</th>\n",
       "      <th>PAGE</th>\n",
       "      <th>CONTENT_CATEGORY</th>\n",
       "      <th>CONTENT_CATEGORY_TOP</th>\n",
       "      <th>CONTENT_CATEGORY_BOTTOM</th>\n",
       "      <th>SITE_ID</th>\n",
       "      <th>ON_SITE_SEARCH_TERM</th>\n",
       "      <th>USER_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>22870354</td>\n",
       "      <td>22,870,354.00</td>\n",
       "      <td>22,870,354.00</td>\n",
       "      <td>22,870,354.00</td>\n",
       "      <td>22,870,354.00</td>\n",
       "      <td>22,870,354.00</td>\n",
       "      <td>22,870,354.00</td>\n",
       "      <td>22,870,354.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2018-07-09 15:04:17.147753216</td>\n",
       "      <td>68.48</td>\n",
       "      <td>2.30</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5,654.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2018-01-01 00:09:17</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2018-04-12 12:51:29</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2,730.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2018-07-11 14:21:00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5,931.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2018-10-04 16:45:36</td>\n",
       "      <td>59.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>8,468.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2018-12-31 23:59:59</td>\n",
       "      <td>1,835.00</td>\n",
       "      <td>68.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>68.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>295.00</td>\n",
       "      <td>11,675.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>170.64</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.71</td>\n",
       "      <td>3,272.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           FEC_EVENT          PAGE  CONTENT_CATEGORY  \\\n",
       "count                       22870354 22,870,354.00     22,870,354.00   \n",
       "mean   2018-07-09 15:04:17.147753216         68.48              2.30   \n",
       "min              2018-01-01 00:09:17          1.00              1.00   \n",
       "25%              2018-04-12 12:51:29          3.00              2.00   \n",
       "50%              2018-07-11 14:21:00         21.00              2.00   \n",
       "75%              2018-10-04 16:45:36         59.00              2.00   \n",
       "max              2018-12-31 23:59:59      1,835.00             68.00   \n",
       "std                              NaN        170.64              1.95   \n",
       "\n",
       "       CONTENT_CATEGORY_TOP  CONTENT_CATEGORY_BOTTOM       SITE_ID  \\\n",
       "count         22,870,354.00            22,870,354.00 22,870,354.00   \n",
       "mean                   1.99                     2.30          2.55   \n",
       "min                    1.00                     1.00          1.00   \n",
       "25%                    2.00                     2.00          2.00   \n",
       "50%                    2.00                     2.00          3.00   \n",
       "75%                    2.00                     2.00          3.00   \n",
       "max                   13.00                    68.00          4.00   \n",
       "std                    0.45                     1.95          0.64   \n",
       "\n",
       "       ON_SITE_SEARCH_TERM       USER_ID  \n",
       "count        22,870,354.00 22,870,354.00  \n",
       "mean                  1.00      5,654.89  \n",
       "min                   1.00          0.00  \n",
       "25%                   1.00      2,730.00  \n",
       "50%                   1.00      5,931.00  \n",
       "75%                   1.00      8,468.00  \n",
       "max                 295.00     11,675.00  \n",
       "std                   0.71      3,272.65  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "dataset.describe(include=\"all\", datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The first thing we have to do is define how to structure the data and separate training and testing.\n",
    "\n",
    "2. Then, as the prediction we have to make is at the user level, we are going to group all their navigation so that we have the same number of rows as the users we have.\n",
    "\n",
    "3. Finally, for each of the explanatory variables that we have (PAGE, CONTENT_CATEGORY, CONTENT_CATEGORY_TOP, CONTENT_CATEGORY_BOTTOM, SITE_ID, ON_SITE_SEARCH_TERM) we will:\n",
    "\n",
    "    - Add their frequency of occurrence of each value of each of the variables,\n",
    "    - calculate the frequency ratio of each possible value in relation to all the values that the variable can take (ie: for PAGE = 1, we add the number of times the user visited PAGE 1 and then divide it by the total visits that made that user to all PAGE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum date is 2018-01-01 00:09:17 and the maximum date is 2018-05-31 23:59:58. \n",
      "\n",
      "Making PAGE\n",
      "Making CONTENT_CATEGORY\n",
      "Making CONTENT_CATEGORY_TOP\n",
      "Making CONTENT_CATEGORY_BOTTOM\n",
      "Making SITE_ID\n",
      "Making ON_SITE_SEARCH_TERM\n",
      "\n",
      "Train shape is (11201, 1824).\n"
     ]
    }
   ],
   "source": [
    "data = dataset[dataset[\"FEC_EVENT\"].dt.month < 6]\n",
    "print(f\"The minimum date is {data['FEC_EVENT'].min()} and the maximum date is \\\n",
    "{data['FEC_EVENT'].max()}. \\n\")\n",
    "train_data = []\n",
    "for c in data.drop([\"USER_ID\", \"FEC_EVENT\"], axis=1).columns:\n",
    "    print(\"Making\", c)\n",
    "    temp = pd.crosstab(data.USER_ID, data[c])\n",
    "    temp.columns = [c + \"_\" + str(v) for v in temp.columns]\n",
    "    train_data.append(temp.apply(lambda x: x / x.sum(), axis=1))\n",
    "train_data = pd.concat(train_data, axis=1)\n",
    "print(f\"\\nTrain shape is {train_data.shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum date is 2018-06-01 00:00:02 and the maximum date is 2018-09-30 23:59:55. \n",
      "\n",
      "Making PAGE\n",
      "Making CONTENT_CATEGORY\n",
      "Making CONTENT_CATEGORY_TOP\n",
      "Making CONTENT_CATEGORY_BOTTOM\n",
      "Making SITE_ID\n",
      "Making ON_SITE_SEARCH_TERM\n",
      "\n",
      "Test shape is (11419, 1489).\n"
     ]
    }
   ],
   "source": [
    "data = dataset[dataset[\"FEC_EVENT\"].dt.month.between(6, 9)]\n",
    "print(f\"The minimum date is {data['FEC_EVENT'].min()} and the maximum date is \\\n",
    "{data['FEC_EVENT'].max()}. \\n\")\n",
    "test_data = []\n",
    "for c in data.drop([\"USER_ID\", \"FEC_EVENT\"], axis=1).columns:\n",
    "    print(\"Making\", c)\n",
    "    temp = pd.crosstab(data.USER_ID, data[c])\n",
    "    temp.columns = [c + \"_\" + str(v) for v in temp.columns]\n",
    "    test_data.append(temp.apply(lambda x: x / x.sum(), axis=1))\n",
    "test_data = pd.concat(test_data, axis=1)\n",
    "print(f\"\\nTest shape is {test_data.shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PAGE_1</th>\n",
       "      <th>PAGE_2</th>\n",
       "      <th>PAGE_3</th>\n",
       "      <th>PAGE_4</th>\n",
       "      <th>PAGE_5</th>\n",
       "      <th>PAGE_6</th>\n",
       "      <th>PAGE_7</th>\n",
       "      <th>PAGE_8</th>\n",
       "      <th>PAGE_9</th>\n",
       "      <th>PAGE_10</th>\n",
       "      <th>...</th>\n",
       "      <th>ON_SITE_SEARCH_TERM_284</th>\n",
       "      <th>ON_SITE_SEARCH_TERM_285</th>\n",
       "      <th>ON_SITE_SEARCH_TERM_286</th>\n",
       "      <th>ON_SITE_SEARCH_TERM_287</th>\n",
       "      <th>ON_SITE_SEARCH_TERM_288</th>\n",
       "      <th>ON_SITE_SEARCH_TERM_289</th>\n",
       "      <th>ON_SITE_SEARCH_TERM_290</th>\n",
       "      <th>ON_SITE_SEARCH_TERM_291</th>\n",
       "      <th>ON_SITE_SEARCH_TERM_292</th>\n",
       "      <th>ON_SITE_SEARCH_TERM_293</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USER_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1824 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         PAGE_1  PAGE_2  PAGE_3  PAGE_4  PAGE_5  PAGE_6  PAGE_7  PAGE_8  \\\n",
       "USER_ID                                                                   \n",
       "0          0.05    0.09    0.02    0.01    0.00    0.00    0.00    0.00   \n",
       "1          0.13    0.12    0.02    0.01    0.00    0.00    0.00    0.00   \n",
       "2          0.03    0.18    0.07    0.02    0.02    0.00    0.00    0.00   \n",
       "3          0.00    0.16    0.10    0.04    0.03    0.01    0.00    0.00   \n",
       "4          0.02    0.09    0.08    0.00    0.03    0.03    0.00    0.00   \n",
       "\n",
       "         PAGE_9  PAGE_10  ...  ON_SITE_SEARCH_TERM_284  \\\n",
       "USER_ID                   ...                            \n",
       "0          0.06     0.18  ...                     0.00   \n",
       "1          0.07     0.18  ...                     0.00   \n",
       "2          0.06     0.13  ...                     0.00   \n",
       "3          0.00     0.00  ...                     0.00   \n",
       "4          0.00     0.24  ...                     0.00   \n",
       "\n",
       "         ON_SITE_SEARCH_TERM_285  ON_SITE_SEARCH_TERM_286  \\\n",
       "USER_ID                                                     \n",
       "0                           0.00                     0.00   \n",
       "1                           0.00                     0.00   \n",
       "2                           0.00                     0.00   \n",
       "3                           0.00                     0.00   \n",
       "4                           0.00                     0.00   \n",
       "\n",
       "         ON_SITE_SEARCH_TERM_287  ON_SITE_SEARCH_TERM_288  \\\n",
       "USER_ID                                                     \n",
       "0                           0.00                     0.00   \n",
       "1                           0.00                     0.00   \n",
       "2                           0.00                     0.00   \n",
       "3                           0.00                     0.00   \n",
       "4                           0.00                     0.00   \n",
       "\n",
       "         ON_SITE_SEARCH_TERM_289  ON_SITE_SEARCH_TERM_290  \\\n",
       "USER_ID                                                     \n",
       "0                           0.00                     0.00   \n",
       "1                           0.00                     0.00   \n",
       "2                           0.00                     0.00   \n",
       "3                           0.00                     0.00   \n",
       "4                           0.00                     0.00   \n",
       "\n",
       "         ON_SITE_SEARCH_TERM_291  ON_SITE_SEARCH_TERM_292  \\\n",
       "USER_ID                                                     \n",
       "0                           0.00                     0.00   \n",
       "1                           0.00                     0.00   \n",
       "2                           0.00                     0.00   \n",
       "3                           0.00                     0.00   \n",
       "4                           0.00                     0.00   \n",
       "\n",
       "         ON_SITE_SEARCH_TERM_293  \n",
       "USER_ID                           \n",
       "0                           0.00  \n",
       "1                           0.00  \n",
       "2                           0.00  \n",
       "3                           0.00  \n",
       "4                           0.00  \n",
       "\n",
       "[5 rows x 1824 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_col = [col for col in train_data if col.startswith(\"PAGE\")]\n",
    "train_data[filter_col].iloc[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have both datasets built, we are going to filter them, keeping the columns that exist in both, in order to train and predict on the same attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape is (11201, 1287).\n",
      "Test shape is (11419, 1287).\n"
     ]
    }
   ],
   "source": [
    "features = list(set(train_data.columns).intersection(set(test_data.columns)))\n",
    "train_data = train_data[features]\n",
    "test_data = test_data[features]\n",
    "print(f\"Train shape is {train_data.shape}.\")\n",
    "print(f\"Test shape is {test_data.shape}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the **conversiones.csv** file that has the target variable and that corresponds to the conversions made during 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mes</th>\n",
       "      <th>anio</th>\n",
       "      <th>USER_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.00</td>\n",
       "      <td>2,018.00</td>\n",
       "      <td>1,410.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.00</td>\n",
       "      <td>2,018.00</td>\n",
       "      <td>10,755.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.00</td>\n",
       "      <td>2,018.00</td>\n",
       "      <td>8,270.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.00</td>\n",
       "      <td>2,018.00</td>\n",
       "      <td>7,558.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.00</td>\n",
       "      <td>2,018.00</td>\n",
       "      <td>10,731.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mes     anio   USER_ID\n",
       "0  7.00 2,018.00  1,410.00\n",
       "1  8.00 2,018.00 10,755.00\n",
       "2  8.00 2,018.00  8,270.00\n",
       "3 10.00 2,018.00  7,558.00\n",
       "4  9.00 2,018.00 10,731.00"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = pd.read_csv(\"../data/conversiones.csv\")\n",
    "target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset again but looking 3 months ahead to align the prediction with the desired time window.\n",
    "\n",
    "* train_data = 2018-01-01/2018-05-31, train_target = 2018-06-01/2018-09-30.\n",
    "* test_data = 2018-06-01/2018-09-30, train_target = 2018-10-01/2018-12-31. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = pd.Series(0, index=train_data.index)\n",
    "train_idx = set(target[target[\"mes\"].between(\n",
    "    6, 9)].USER_ID.unique()).intersection(set(train_data.index))\n",
    "train_target.loc[list(train_idx)] = 1\n",
    "\n",
    "test_target = pd.Series(0, index=test_data.index)\n",
    "test_idx = set(target[target[\"mes\"] > 9].USER_ID.unique()\n",
    "               ).intersection(set(test_data.index))\n",
    "test_target.loc[list(test_idx)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in train\n",
      "0    10704\n",
      "1      497\n",
      "dtype: int64\n",
      "\n",
      "Class distribution in test\n",
      "0    11033\n",
      "1      386\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Class distribution in train\")\n",
    "print(train_target.value_counts())\n",
    "\n",
    "print(\"\\nClass distribution in test\")\n",
    "print(test_target.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = LGBMClassifier(\n",
    "    random_state=0).fit(train_data, train_target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithms in scikit-learn API can predict with a confidence score (in some cases) or predict the target directly (by default it considers a cutoff point of 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.96906420e-01, 3.09358022e-03],\n",
       "       [9.99279814e-01, 7.20186469e-04],\n",
       "       [9.99514361e-01, 4.85639499e-04],\n",
       "       [9.83401599e-01, 1.65984007e-02],\n",
       "       [9.96897116e-01, 3.10288367e-03],\n",
       "       [9.97671708e-01, 2.32829226e-03],\n",
       "       [9.99233649e-01, 7.66351400e-04],\n",
       "       [9.92498929e-01, 7.50107099e-03],\n",
       "       [9.96851419e-01, 3.14858143e-03],\n",
       "       [9.99306174e-01, 6.93826348e-04]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.predict_proba(test_data)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.predict(test_data)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9635694894474122\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score: {}\".format(accuracy_score(test_target,\n",
    "learner.predict(test_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix(rows=real, columns=pred)\n",
      "        NO  YES\n",
      "NO   10994   39\n",
      "YES    377    9\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix(rows=real, columns=pred)\")\n",
    "print(\n",
    "    pd.DataFrame(\n",
    "        confusion_matrix(\n",
    "            test_target,\n",
    "            learner.predict(test_data)\n",
    "            ), columns=['NO', 'YES'], index=['NO', 'YES']\n",
    "            )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Accuracy_**:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://static.wixstatic.com/media/02a1ae_32cad84eaf3348059a8996d1b0f88627~mv2.jpg/v1/fill/w_597,h_416,al_c,q_90/02a1ae_32cad84eaf3348059a8996d1b0f88627~mv2.jpg\" width=\"300\" height=\"200\">\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "$$ Accuracy = \\frac{TP+TN}{TP+TN+FP+FN} $$\n",
    "\n",
    "**_Precision_**:\n",
    "\n",
    "What proportion of positive identifications was actually correct?\n",
    "\n",
    "Let's calculate precision for our model:\n",
    "\n",
    "$$ Precision = \\frac{TP}{TP+FP} = \\frac{9}{9+39} = 0.19 $$\n",
    "\n",
    "Our model has a precision of 0.19 or, in other words, when it predicts \"YES\", it is correct 19% of the time.\n",
    "\n",
    "**_Recall_**:\n",
    "\n",
    "What proportion of actual positives was identified correctly?\n",
    "\n",
    "$$ Recall = \\frac{TP}{TP+FN} = \\frac{9}{9+377} = 0.02 $$\n",
    "\n",
    "Our model has a recall of 0.02 or, in other words, it correctly identifies 2% of all \"YES\".\n",
    "\n",
    "**_F1 Score_**:\n",
    "\n",
    "The F1 score is defined as the harmonic mean of precision and recall.\n",
    "\n",
    "$$ F1 Score = 2 * \\frac{Precision*Recall}{Precision+Recall} = 2 * \\frac{0.19*0.02}{0.19+0.02} = 0.04 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          NO       0.97      1.00      0.98     11033\n",
      "         YES       0.19      0.02      0.04       386\n",
      "\n",
      "    accuracy                           0.96     11419\n",
      "   macro avg       0.58      0.51      0.51     11419\n",
      "weighted avg       0.94      0.96      0.95     11419\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(\n",
    "    test_target,\n",
    "    learner.predict(test_data),\n",
    "    target_names=['NO', 'YES'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem here is that an accuracy of 96% sounds like a great result, whereas the model performs very poorly. In conclusion: accuracy is not a good metric to use when you have class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversampling increases the number of minority class members in the training set.\n",
    "\n",
    "To perform the oversampling we use the library [imbalanced-learn](https://imbalanced-learn.org/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance = RandomOverSampler(random_state=0)\n",
    "train_data, train_target = balance.fit_resample(train_data, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in train after balance\n",
      "0    10704\n",
      "1    10704\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Class distribution in train after balance\")\n",
    "print(train_target.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = LGBMClassifier(\n",
    "    random_state=0).fit(train_data, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix(rows=real, columns=pred)\n",
      "        NO  YES\n",
      "NO   10433  600\n",
      "YES    268  118\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix(rows=real, columns=pred)\")\n",
    "print(\n",
    "    pd.DataFrame(\n",
    "        confusion_matrix(\n",
    "            test_target,\n",
    "            learner.predict(test_data)\n",
    "            ), columns=['NO', 'YES'], index=['NO', 'YES']\n",
    "            )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          NO       0.97      0.95      0.96     11033\n",
      "         YES       0.16      0.31      0.21       386\n",
      "\n",
      "    accuracy                           0.92     11419\n",
      "   macro avg       0.57      0.63      0.59     11419\n",
      "weighted avg       0.95      0.92      0.93     11419\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(\n",
    "    test_target,\n",
    "    learner.predict(test_data),\n",
    "    target_names=['NO', 'YES'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection is the process of reducing the number of input variables.\n",
    "\n",
    "It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model.\n",
    "\n",
    "Statistical-based feature selection methods involve evaluating the relationship between each input variable and the target variable using statistics and selecting those input variables that have the strongest relationship with the target variable.\n",
    "\n",
    "Here we use [ANOVA F-value](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('fs', SelectKBest(score_func=f_classif, k=300)),\n",
    "    ('clf', LGBMClassifier(random_state=0))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = pipeline.fit(train_data, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix(rows=real, columns=pred)\n",
      "        NO  YES\n",
      "NO   10412  621\n",
      "YES    265  121\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix(rows=real, columns=pred)\")\n",
    "print(\n",
    "    pd.DataFrame(\n",
    "        confusion_matrix(\n",
    "            test_target,\n",
    "            learner.predict(test_data)\n",
    "            ), columns=['NO', 'YES'], index=['NO', 'YES']\n",
    "            )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          NO       0.98      0.94      0.96     11033\n",
      "         YES       0.16      0.31      0.21       386\n",
      "\n",
      "    accuracy                           0.92     11419\n",
      "   macro avg       0.57      0.63      0.59     11419\n",
      "weighted avg       0.95      0.92      0.93     11419\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(\n",
    "    test_target,\n",
    "    learner.predict(test_data),\n",
    "    target_names=['NO', 'YES'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering is the process of selecting, manipulating, and transforming raw data into features. In order to make machine learning work well on new tasks, it might be necessary to design and train better features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = LGBMClassifier(\n",
    "    random_state=0).fit(train_data, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAGE_65               0.02\n",
      "PAGE_41               0.02\n",
      "PAGE_39               0.01\n",
      "PAGE_87               0.01\n",
      "PAGE_5                0.01\n",
      "PAGE_2                0.01\n",
      "CONTENT_CATEGORY_16   0.01\n",
      "PAGE_20               0.01\n",
      "PAGE_69               0.01\n",
      "PAGE_110              0.01\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "fi = pd.Series(\n",
    "    learner.feature_importances_ / learner.feature_importances_.sum(),\n",
    "    index=train_data.columns\n",
    ")\n",
    "print(fi.sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"FI\"] = train_data['PAGE_65'] + train_data['PAGE_41']\n",
    "test_data[\"FI\"] = test_data['PAGE_65'] + test_data['PAGE_41']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = LGBMClassifier(\n",
    "    random_state=0).fit(train_data, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix(rows=real, columns=pred)\n",
      "        NO  YES\n",
      "NO   10394  639\n",
      "YES    245  141\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix(rows=real, columns=pred)\")\n",
    "print(\n",
    "    pd.DataFrame(\n",
    "        confusion_matrix(\n",
    "            test_target,\n",
    "            learner.predict(test_data)\n",
    "            ), columns=['NO', 'YES'], index=['NO', 'YES']\n",
    "            )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          NO       0.98      0.94      0.96     11033\n",
      "         YES       0.18      0.37      0.24       386\n",
      "\n",
      "    accuracy                           0.92     11419\n",
      "   macro avg       0.58      0.65      0.60     11419\n",
      "weighted avg       0.95      0.92      0.93     11419\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(\n",
    "    test_target,\n",
    "    learner.predict(test_data),\n",
    "    target_names=['NO', 'YES'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_conversions = target[target[\"mes\"] < 6].groupby(\n",
    "    \"USER_ID\")[\"USER_ID\"].count().reset_index(name=\"CANT_CONV\").set_index(\n",
    "        'USER_ID')\n",
    "train_data = train_data.merge(\n",
    "    train_conversions, how=\"left\", left_index=True, right_index=True).fillna(0)\n",
    "train_data[\"CANT_CONV\"] = np.where(train_data[\"CANT_CONV\"] > 0, 1, 0)\n",
    "\n",
    "test_conversions = target[target[\"mes\"] < 10].groupby(\n",
    "    \"USER_ID\")[\"USER_ID\"].count().reset_index(name=\"CANT_CONV\").set_index(\n",
    "        'USER_ID')\n",
    "test_data = test_data.merge(\n",
    "    test_conversions, how=\"left\", left_index=True, right_index=True).fillna(0)\n",
    "test_data[\"CANT_CONV\"] = np.where(test_data[\"CANT_CONV\"] > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = LGBMClassifier(\n",
    "    random_state=0).fit(train_data, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix(rows=real, columns=pred)\n",
      "        NO  YES\n",
      "NO   10682  351\n",
      "YES    331   55\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix(rows=real, columns=pred)\")\n",
    "print(\n",
    "    pd.DataFrame(\n",
    "        confusion_matrix(\n",
    "            test_target,\n",
    "            learner.predict(test_data)\n",
    "            ), columns=['NO', 'YES'], index=['NO', 'YES']\n",
    "            )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          NO       0.97      0.97      0.97     11033\n",
      "         YES       0.14      0.14      0.14       386\n",
      "\n",
      "    accuracy                           0.94     11419\n",
      "   macro avg       0.55      0.56      0.55     11419\n",
      "weighted avg       0.94      0.94      0.94     11419\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(\n",
    "    test_target,\n",
    "    learner.predict(test_data),\n",
    "    target_names=['NO', 'YES'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop(\"CANT_CONV\", inplace=True, axis=1)\n",
    "test_data.drop(\"CANT_CONV\", inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could try other things, like calculating the time spent on each page, but we leave that as homework :P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many machine learning algorithms are capable of predicting a probability or scoring of class membership, and this must be interpreted before it can be mapped to a crisp class label. This is achieved by using a threshold, such as 0.5, where all values equal or greater than the threshold are mapped to one class and all other values are mapped to another class.\n",
    "\n",
    "For those classification problems that have a severe class imbalance, the default threshold can result in poor performance. As such, a simple and straightforward approach to improving the performance of a classifier that predicts probabilities on an imbalanced classification problem is to tune the threshold used to map probabilities to class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import arange\n",
    "from numpy import argmax\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_labels(pos_probs, threshold):\n",
    "\treturn (pos_probs >= threshold).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = LGBMClassifier(\n",
    "    random_state=0).fit(train_data, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold=0.496, F-Score=0.24253\n"
     ]
    }
   ],
   "source": [
    "thresholds = arange(0, 1, 0.001)\n",
    "scores = [f1_score(test_target, to_labels(learner.predict_proba(test_data)[:, -1], t)) for t in thresholds]\n",
    "ix = argmax(scores)\n",
    "print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix(rows=real, columns=pred)\n",
      "        NO  YES\n",
      "NO   10165  868\n",
      "YES    227  159\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix(rows=real, columns=pred)\")\n",
    "print(\n",
    "    pd.DataFrame(\n",
    "        confusion_matrix(\n",
    "            test_target,\n",
    "            np.where(learner.predict_proba(test_data)[:, -1] >= 0.40, 1, 0)\n",
    "            ), columns=['NO', 'YES'], index=['NO', 'YES']\n",
    "            )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          NO       0.98      0.92      0.95     11033\n",
      "         YES       0.15      0.41      0.23       386\n",
      "\n",
      "    accuracy                           0.90     11419\n",
      "   macro avg       0.57      0.67      0.59     11419\n",
      "weighted avg       0.95      0.90      0.92     11419\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(\n",
    "    test_target,\n",
    "    np.where(learner.predict_proba(test_data)[:, -1] >= 0.40, 1, 0),\n",
    "    target_names=['NO', 'YES'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coming soon...\n",
    "\n",
    "* Overfitting,\n",
    "* bias–variance tradeoff,\n",
    "* cross validation,\n",
    "* hyperparameter optimization,\n",
    "* model selection,\n",
    "* stacking,\n",
    "* and much more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('wds-9IgDSWEo-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb1d205f21b70400a7534dc1c0f579143c905efa8f17e985bd1c77bbe8f5c0bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
